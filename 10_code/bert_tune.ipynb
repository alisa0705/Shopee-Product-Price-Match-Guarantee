{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from torch import nn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import *\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_path = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'\n",
    "training_dataset =pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')\n",
    "testing_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process titles (remove punctuation, stop words, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# remove numbers\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def word_tokenize(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    return filtered_sentence\n",
    "\n",
    "# remove all preprocessing\n",
    "def remove_all_preprocessing(text):\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "# apply all preprocessing\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "testing_dataset['title'] = testing_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "\n",
    "# get rid of \\\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.replace('\\\\', ''))\n",
    "# lower case\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with base BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 19:35:46.590965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 19:35:50.916083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-15 19:35:53.804918: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "920it [03:54,  3.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m idx,txt \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(training_dataset[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     11\u001b[0m   encoded_input \u001b[39m=\u001b[39m tokenizer(txt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m   output \u001b[39m=\u001b[39m model(encoded_input)\n\u001b[1;32m     13\u001b[0m   bert_title_vectors[idx]\u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:432\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    431\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 432\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:1114\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m   1071\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1072\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1093\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m   1094\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[39m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1115\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1116\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1117\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1118\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1119\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1120\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1121\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1122\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1123\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1124\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1125\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1126\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1127\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1128\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1129\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:432\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    431\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 432\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:870\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    868\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 870\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    871\u001b[0m     hidden_states\u001b[39m=\u001b[39;49membedding_output,\n\u001b[1;32m    872\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    873\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    874\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    875\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    876\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    877\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    878\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    879\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    880\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    881\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    882\u001b[0m )\n\u001b[1;32m    884\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(hidden_states\u001b[39m=\u001b[39msequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:562\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    558\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m    560\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[i] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    563\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    565\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    566\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    567\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    568\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    570\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:472\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    461\u001b[0m     hidden_states: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    473\u001b[0m         input_tensor\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    474\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    475\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    476\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    477\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    478\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    479\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    480\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    482\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:388\u001b[0m, in \u001b[0;36mTFBertAttention.call\u001b[0;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     input_tensor: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 388\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    389\u001b[0m         hidden_states\u001b[39m=\u001b[39;49minput_tensor,\n\u001b[1;32m    390\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    391\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    392\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    393\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    394\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    395\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    396\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_output(\n\u001b[1;32m    399\u001b[0m         hidden_states\u001b[39m=\u001b[39mself_outputs[\u001b[39m0\u001b[39m], input_tensor\u001b[39m=\u001b[39minput_tensor, training\u001b[39m=\u001b[39mtraining\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    401\u001b[0m     \u001b[39m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:318\u001b[0m, in \u001b[0;36mTFBertSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    314\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    316\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# (batch size, num_heads, seq_len_q, seq_len_k)\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m attention_scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer, transpose_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    319\u001b[0m dk \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_att_head_size, dtype\u001b[39m=\u001b[39mattention_scores\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    320\u001b[0m attention_scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdivide(attention_scores, dk)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:3719\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3716\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[1;32m   3717\u001b[0m         a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   3718\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3719\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mbatch_mat_mul_v2(\n\u001b[1;32m   3720\u001b[0m         a, b, adj_x\u001b[39m=\u001b[39;49madjoint_a, adj_y\u001b[39m=\u001b[39;49madjoint_b, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   3722\u001b[0m \u001b[39m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[39m# the matrix and use transpose instead. Conj() is a noop for real\u001b[39;00m\n\u001b[1;32m   3724\u001b[0m \u001b[39m# matrices.\u001b[39;00m\n\u001b[1;32m   3725\u001b[0m \u001b[39mif\u001b[39;00m adjoint_a:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:1570\u001b[0m, in \u001b[0;36mbatch_mat_mul_v2\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1569\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1570\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1571\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mBatchMatMulV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y, \u001b[39m\"\u001b[39;49m\u001b[39madj_x\u001b[39;49m\u001b[39m\"\u001b[39;49m, adj_x, \u001b[39m\"\u001b[39;49m\u001b[39madj_y\u001b[39;49m\u001b[39m\"\u001b[39;49m, adj_y)\n\u001b[1;32m   1572\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1573\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name='cahya/bert-base-indonesian-522M'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_title_vectors = np.zeros((training_dataset.shape[0],768))\n",
    "\n",
    "for idx,txt in tqdm(enumerate(training_dataset['title'])):\n",
    "  encoded_input = tokenizer(txt, return_tensors='tf')\n",
    "  output = model(encoded_input)\n",
    "  bert_title_vectors[idx]= output['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_6\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>, <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[    3, 20185,  1632,  8476, 12395,     1,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2]], dtype=int32)>, <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m idx, txt \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(training_dataset[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     28\u001b[0m     encoded_input \u001b[39m=\u001b[39m tokenizer(txt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39mmax_length, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m     output \u001b[39m=\u001b[39m model(encoded_input)\n\u001b[1;32m     30\u001b[0m     bert_title_vectors[idx] \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/keras/engine/input_spec.py:219\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs to a layer should be tensors. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m) as input for layer \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(input_spec):\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m expects \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_spec)\u001b[39m}\u001b[39;00m\u001b[39m input(s),\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but it received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m input tensors. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs received: \u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m input_index, (x, spec) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(inputs, input_spec)):\n\u001b[1;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Layer \"model_6\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>, <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[    3, 20185,  1632,  8476, 12395,     1,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2,     2,     2,     2,     2,     2,     2,     2,\n            2,     2]], dtype=int32)>, <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>]"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "max_length = 128\n",
    "num_classes = 10\n",
    "model_name='cahya/bert-base-indonesian-522M'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_layer = model(input_ids, attention_mask=attention_mask)[0]\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "batch_norm_layer = tf.keras.layers.BatchNormalization()(pooling_layer)\n",
    "dense_layer = tf.keras.layers.Dense(256, activation='relu')(batch_norm_layer)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)(dense_layer)\n",
    "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "bert_title_vectors = np.zeros((training_dataset.shape[0], 768))\n",
    "\n",
    "for idx, txt in tqdm(enumerate(training_dataset['title'])):\n",
    "    encoded_input = tokenizer(txt, return_tensors='tf', max_length=max_length, padding='max_length', truncation=True)\n",
    "    output = model(encoded_input)\n",
    "    bert_title_vectors[idx] = output[0][0].numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check encoding from Advanced BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05237605  0.00266704 -0.0351623  ... -0.0220446   0.03120822\n",
      "   0.0482746 ]\n",
      " [ 0.03168393  0.0207246  -0.01772413 ... -0.02363313  0.00672482\n",
      "  -0.06600535]\n",
      " [-0.02553968  0.02782196  0.01990272 ... -0.0465625  -0.01983832\n",
      "   0.01210797]\n",
      " ...\n",
      " [-0.00043891  0.0663081  -0.00434749 ...  0.04577914  0.02965026\n",
      "  -0.03386611]\n",
      " [-0.02243138  0.02246639  0.05454137 ...  0.01867248  0.01682098\n",
      "   0.00408043]\n",
      " [-0.03517014  0.02272914  0.00684843 ... -0.00679976 -0.01856173\n",
      "  -0.00943737]]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(training_dataset['title'])\n",
    "print(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add layers to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(weights='distance')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn_model.fit(embeddings, training_dataset['label_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_dataset['label_group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, Accuracy: 0.498978102189781\n",
      "k = 1, f1: 0.4807792604732177\n",
      "k = 2, Accuracy: 0.38306569343065694\n",
      "k = 2, f1: 0.35805505780994695\n",
      "k = 3, Accuracy: 0.3375182481751825\n",
      "k = 3, f1: 0.31057035183397974\n",
      "k = 5, Accuracy: 0.3005839416058394\n",
      "k = 5, f1: 0.2676876029260309\n",
      "k = 10, Accuracy: 0.24744525547445256\n",
      "k = 10, f1: 0.20623973530342002\n",
      "k = 25, Accuracy: 0.194014598540146\n",
      "k = 25, f1: 0.14738019756662057\n",
      "k = 50, Accuracy: 0.16583941605839417\n",
      "k = 50, f1: 0.11615758877312352\n",
      "k = 100, Accuracy: 0.13605839416058393\n",
      "k = 100, f1: 0.08739168649665359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "acc_dict = {}\n",
    "for k in [1, 2, 3, 5, 10, 25, 50, 100]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"k = {k}, Accuracy: {accuracy}\")\n",
    "    print(f\"k = {k}, f1: {f1}\")\n",
    "    acc_dict[k] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f88e7b950>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_dict.keys(), acc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38306569343065694\n",
      "F1 score: 0.35805505780994695\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'Transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msentence-transformers/distiluse-base-multilingual-cased-v2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m word_embedding_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mTransformer(\u001b[39m'\u001b[39m\u001b[39msentence-transformers/distiluse-base-multilingual-cased-v2\u001b[39m\u001b[39m'\u001b[39m, max_seq_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n\u001b[1;32m      5\u001b[0m pooling_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mPooling(word_embedding_model\u001b[39m.\u001b[39mget_word_embedding_dimension())\n\u001b[1;32m      6\u001b[0m dense_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mDense(in_features\u001b[39m=\u001b[39mpooling_model\u001b[39m.\u001b[39mget_sentence_embedding_dimension(), out_features\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, activation_function\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mTanh())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'Transformer'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "\n",
    "word_embedding_model = models.Transformer('sentence-transformers/distiluse-base-multilingual-cased-v2', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "#Our sentences we like to encode\n",
    "# sentences = ['This framework generates embeddings for each input sentence',\n",
    "#     'Sentences are passed as a list of string.',\n",
    "#     'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(training_dataset.title)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(training_dataset.title, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
