{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import *\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_path = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'\n",
    "training_dataset =pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')\n",
    "testing_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# remove numbers\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def word_tokenize(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    return filtered_sentence\n",
    "\n",
    "# remove all preprocessing\n",
    "def remove_all_preprocessing(text):\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "# apply all preprocessing\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "testing_dataset['title'] = testing_dataset['title'].apply(lambda x: remove_all_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of \\\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.replace('\\\\', ''))\n",
    "# lower case\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05237605  0.00266704 -0.0351623  ... -0.0220446   0.03120822\n",
      "   0.0482746 ]\n",
      " [ 0.03168393  0.0207246  -0.01772413 ... -0.02363313  0.00672482\n",
      "  -0.06600535]\n",
      " [-0.02553968  0.02782196  0.01990272 ... -0.0465625  -0.01983832\n",
      "   0.01210797]\n",
      " ...\n",
      " [-0.00043891  0.0663081  -0.00434749 ...  0.04577914  0.02965026\n",
      "  -0.03386611]\n",
      " [-0.02243138  0.02246639  0.05454137 ...  0.01867248  0.01682098\n",
      "   0.00408043]\n",
      " [-0.03517014  0.02272914  0.00684843 ... -0.00679976 -0.01856173\n",
      "  -0.00943737]]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(training_dataset['title'])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "word_embedding_model = models.Transformer('sentence-transformers/distiluse-base-multilingual-cased-v2', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v2 were not used when initializing BertModel: ['transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v2 and are newly initialized: ['encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_version = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
    "model = BertModel.from_pretrained(bert_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_training_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m progress_bar_train \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_training_steps))\n\u001b[1;32m      4\u001b[0m progress_bar_eval \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_epochs \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(eval_dataloader)))\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_training_steps' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(25 * len(eval_dataloader)))\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      progress_bar_train.update(1)\n",
    "\n",
    "  model.eval()\n",
    "  for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar_eval.update(1)\n",
    "    \n",
    "  print(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
