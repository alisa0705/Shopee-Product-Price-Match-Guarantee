{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras import applications, layers, losses, optimizers, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textwrap import wrap\n",
    "from transformers import ResNetConfig, ResNetModel\n",
    "from transformers import AutoImageProcessor, TFResNetForImageClassification\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFResNetForImageClassification.\n",
      "\n",
      "All the layers of TFResNetForImageClassification were initialized from the model checkpoint at microsoft/resnet-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = TFResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/'\n",
    "PATH_TO_IMG = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images/'\n",
    "PATH_TO_TEST = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/workspaces/Shopee-Price-Match-Guarantee/30_results/HG_ResNet50/checkpoints/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(train_ds.class_names)\n",
    "\n",
    "\n",
    "# import the ResNet50 model from keras\n",
    "\n",
    "resnet = Sequential()\n",
    "pretrained_model = applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3), # change from 256 to 224\n",
    "    pooling='avg',\n",
    "    classes=num_classes,\n",
    "    classifier_activation='softmax',\n",
    ")\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "resnet.add(pretrained_model)\n",
    "# add batch normalization layer\n",
    "resnet.add(BatchNormalization())\n",
    "# flatten the output\n",
    "resnet.add(Flatten())\n",
    "# add dropout layer\n",
    "resnet.add(Dropout(0.5))\n",
    "# add linear layer for reduction\n",
    "resnet.add(Dense(512, activation='linear'))\n",
    "# add classification layer\n",
    "resnet.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5 \n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFResNetForImageClassification.\n",
      "\n",
      "All the layers of TFResNetForImageClassification were initialized from the model checkpoint at microsoft/resnet-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFResNetForImageClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from datasets import load_dataset\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(train_ds.class_names)\n",
    "\n",
    "# import the ResNet50 model from transformers\n",
    "pretrained_model = TFResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77847df634cc4353820d2bce2ddb2ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/Users/alisa/.cache/huggingface/datasets/imagefolder/train_images-82e4eb5df6582b4a/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6d0532c7f640af916bf19ba24582ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = load_dataset(PATH_TO_IMG) # load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "All model checkpoint layers were used when initializing TFResNetForImageClassification.\n",
      "\n",
      "All the layers of TFResNetForImageClassification were initialized from the model checkpoint at microsoft/resnet-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = TFResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "inputs = image_processor(image[\"train\"][\"image\"], return_tensors=\"tf\")\n",
    "# inputs = image\n",
    "logits = model(**inputs).logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = int(tf.math.argmax(logits, axis=-1))\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:2709\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2709\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:2694\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2693\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2694\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2695\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2696\u001b[0m )\n\u001b[1;32m   2697\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py:634\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    632\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    635\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    636\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py:408\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    407\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_column(pa_table)\n\u001b[1;32m    409\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py:447\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_column\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    446\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_column(pa_table)\n\u001b[0;32m--> 447\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_features_decoder\u001b[39m.\u001b[39;49mdecode_column(column, pa_table\u001b[39m.\u001b[39;49mcolumn_names[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    448\u001b[0m     \u001b[39mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py:228\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_column\u001b[39m(\u001b[39mself\u001b[39m, column: \u001b[39mlist\u001b[39m, column_name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mdecode_column(column, column_name) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39melse\u001b[39;00m column\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/features/features.py:1886\u001b[0m, in \u001b[0;36mFeatures.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_column\u001b[39m(\u001b[39mself\u001b[39m, column: \u001b[39mlist\u001b[39m, column_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1874\u001b[0m     \u001b[39m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \n\u001b[1;32m   1876\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[39m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m-> 1886\u001b[0m         [decode_nested_example(\u001b[39mself\u001b[39m[column_name], value) \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1887\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1888\u001b[0m         \u001b[39melse\u001b[39;00m column\n\u001b[1;32m   1889\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/features/features.py:1886\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_column\u001b[39m(\u001b[39mself\u001b[39m, column: \u001b[39mlist\u001b[39m, column_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1874\u001b[0m     \u001b[39m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \n\u001b[1;32m   1876\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[39m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m-> 1886\u001b[0m         [decode_nested_example(\u001b[39mself\u001b[39;49m[column_name], value) \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1887\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1888\u001b[0m         \u001b[39melse\u001b[39;00m column\n\u001b[1;32m   1889\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/features/features.py:1308\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1306\u001b[0m     \u001b[39m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m schema\u001b[39m.\u001b[39mdecode:\n\u001b[0;32m-> 1308\u001b[0m         \u001b[39mreturn\u001b[39;00m schema\u001b[39m.\u001b[39;49mdecode_example(obj, token_per_repo_id\u001b[39m=\u001b[39;49mtoken_per_repo_id)\n\u001b[1;32m   1309\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/features/image.py:178\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 178\u001b[0m image\u001b[39m.\u001b[39;49mload()  \u001b[39m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/PIL/ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[1;32m    256\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 257\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image[\"train\"][\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFResNetForImageClassification.\n",
      "\n",
      "All the layers of TFResNetForImageClassification were initialized from the model checkpoint at microsoft/resnet-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Layer tf_res_net_for_image_classification_2 is not connected, no input to return.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m pretrained_model \u001b[39m=\u001b[39m TFResNetForImageClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/resnet-50\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# remove the classification head\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pretrained_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39mpretrained_model\u001b[39m.\u001b[39;49minput, outputs\u001b[39m=\u001b[39mpretrained_model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39moutput)\n\u001b[1;32m     15\u001b[0m resnet \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m     16\u001b[0m resnet\u001b[39m.\u001b[39madd(pretrained_model)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:2061\u001b[0m, in \u001b[0;36mLayer.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[39m\"\"\"Retrieves the input tensor(s) of a layer.\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \n\u001b[1;32m   2050\u001b[0m \u001b[39mOnly applicable if the layer has exactly one input,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[39m  AttributeError: If no inbound nodes are found.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inbound_nodes:\n\u001b[0;32m-> 2061\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2062\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is not connected, no input to return.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_node_attribute_at_index(\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput_tensors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer tf_res_net_for_image_classification_2 is not connected, no input to return."
     ]
    }
   ],
   "source": [
    "from transformers import TFResNetForImageClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(train_ds.class_names)\n",
    "\n",
    "# import the ResNet50 model from transformers\n",
    "pretrained_model = TFResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "# remove the classification head\n",
    "pretrained_model = tf.keras.Model(inputs=pretrained_model.input, outputs=pretrained_model.layers[-3].output)\n",
    "\n",
    "resnet = Sequential()\n",
    "resnet.add(pretrained_model)\n",
    "\n",
    "# add batch normalization layer\n",
    "resnet.add(BatchNormalization())\n",
    "# flatten the output\n",
    "resnet.add(Flatten())\n",
    "# add dropout layer\n",
    "resnet.add(Dropout(0.5))\n",
    "# add linear layer for reduction\n",
    "resnet.add(Dense(512, activation='linear'))\n",
    "# add classification layer\n",
    "resnet.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "resnet.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[get_f1])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    # monitor='val_loss',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# load checkpoint if exist\n",
    "# resnet.load_weights(checkpoint_filepath)\n",
    "\n",
    "resnet.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
