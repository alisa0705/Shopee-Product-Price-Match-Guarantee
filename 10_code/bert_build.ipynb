{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "# copyfile(src = \"../input/shopee-external-models/tokenization.py\", dst = \"../working/tokenization.py\")\n",
    "from bert import tokenization\n",
    "# import tokenization\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 11014 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 17:28:20.434446: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 125018112 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --preserve_unused_tokens before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 183\u001b[0m\n\u001b[1;32m    173\u001b[0m     history \u001b[39m=\u001b[39m bert_model\u001b[39m.\u001b[39mfit(x_train, y_train,\n\u001b[1;32m    174\u001b[0m                              validation_data \u001b[39m=\u001b[39m (x_val, y_val),\n\u001b[1;32m    175\u001b[0m                              epochs \u001b[39m=\u001b[39m EPOCHS, \n\u001b[1;32m    176\u001b[0m                              callbacks \u001b[39m=\u001b[39m [checkpoint],\n\u001b[1;32m    177\u001b[0m                              batch_size \u001b[39m=\u001b[39m BATCH_SIZE,\n\u001b[1;32m    178\u001b[0m                              verbose \u001b[39m=\u001b[39m VERBOSE)\n\u001b[1;32m    182\u001b[0m df, N_CLASSES, x_train, x_val, y_train, y_val \u001b[39m=\u001b[39m read_and_preprocess()\n\u001b[0;32m--> 183\u001b[0m load_train_and_evaluate(x_train, x_val, y_train, y_val)\n",
      "Cell \u001b[0;32mIn[6], line 159\u001b[0m, in \u001b[0;36mload_train_and_evaluate\u001b[0;34m(x_train, x_val, y_train, y_val)\u001b[0m\n\u001b[1;32m    157\u001b[0m do_lower_case \u001b[39m=\u001b[39m bert_layer\u001b[39m.\u001b[39mresolved_object\u001b[39m.\u001b[39mdo_lower_case\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    158\u001b[0m tokenizer \u001b[39m=\u001b[39m tokenization\u001b[39m.\u001b[39mFullTokenizer(vocab_file, do_lower_case)\n\u001b[0;32m--> 159\u001b[0m x_train \u001b[39m=\u001b[39m bert_encode(x_train[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues, tokenizer, max_len \u001b[39m=\u001b[39;49m \u001b[39m70\u001b[39;49m)\n\u001b[1;32m    160\u001b[0m x_val \u001b[39m=\u001b[39m bert_encode(x_val[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, tokenizer, max_len \u001b[39m=\u001b[39m \u001b[39m70\u001b[39m)\n\u001b[1;32m    161\u001b[0m y_train \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mvalues\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mbert_encode\u001b[0;34m(texts, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     32\u001b[0m all_segments \u001b[39m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts:\n\u001b[0;32m---> 35\u001b[0m     text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[1;32m     37\u001b[0m     text \u001b[39m=\u001b[39m text[:max_len\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     38\u001b[0m     input_sequence \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m text \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/bert/tokenization.py:192\u001b[0m, in \u001b[0;36mFullTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m    191\u001b[0m   split_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 192\u001b[0m   \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text):\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m preserve_token(token, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab):\n\u001b[1;32m    194\u001b[0m       split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/bert/tokenization.py:237\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    235\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m orig_tokens:\n\u001b[0;32m--> 237\u001b[0m   \u001b[39mif\u001b[39;00m preserve_token(token, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab):\n\u001b[1;32m    238\u001b[0m     split_tokens\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m    239\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/bert/tokenization.py:41\u001b[0m, in \u001b[0;36mpreserve_token\u001b[0;34m(token, vocab)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreserve_token\u001b[39m(token, vocab):\n\u001b[1;32m     40\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns True if the token should forgo tokenization and be preserved.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m FLAGS\u001b[39m.\u001b[39;49mpreserve_unused_tokens:\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     43\u001b[0m   \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/absl/flags/_flagvalues.py:481\u001b[0m, in \u001b[0;36mFlagValues.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mreturn\u001b[39;00m fl[name]\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m   \u001b[39mraise\u001b[39;00m _exceptions\u001b[39m.\u001b[39mUnparsedFlagAccessError(\n\u001b[1;32m    482\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mTrying to access flag --\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m before flags were parsed.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m name)\n",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --preserve_unused_tokens before flags were parsed."
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "# Seed\n",
    "SEED = 123\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "LR = 0.00001\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def read_and_preprocess():\n",
    "    df = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')\n",
    "    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "    df['matches'] = df['label_group'].map(tmp)\n",
    "    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "    encoder = LabelEncoder()\n",
    "    df['label_group'] = encoder.fit_transform(df['label_group'])\n",
    "    N_CLASSES = df['label_group'].nunique()\n",
    "    print(f'We have {N_CLASSES} classes')\n",
    "    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n",
    "    return df, N_CLASSES, x_train, x_val, y_train, y_val\n",
    "\n",
    "# Return tokens, masks and segments from a text array or series\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "# Function to build bert model\n",
    "def build_bert_model(bert_layer, max_len = 512):\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    x = margin([clf_output, label])\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
    "                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "def load_train_and_evaluate(x_train, x_val, y_train, y_val):\n",
    "    seed_everything(SEED)\n",
    "    # Load BERT from the Tensorflow Hub\n",
    "    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "    bert_layer = hub.KerasLayer(module_url, trainable = True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n",
    "    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "    # Add targets to train and val\n",
    "    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
    "    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
    "    bert_model = build_bert_model(bert_layer, max_len = 70)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Bert_{SEED}.h5', \n",
    "                                                    monitor = 'val_loss', \n",
    "                                                    verbose = VERBOSE, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    history = bert_model.fit(x_train, y_train,\n",
    "                             validation_data = (x_val, y_val),\n",
    "                             epochs = EPOCHS, \n",
    "                             callbacks = [checkpoint],\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             verbose = VERBOSE)\n",
    "    \n",
    "    \n",
    "\n",
    "df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n",
    "load_train_and_evaluate(x_train, x_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnparsedFlagAccessError: Trying to access flag --preserve_unused_tokens before flags were parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
