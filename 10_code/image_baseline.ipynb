{"cells":[{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:26:53.934115Z","iopub.status.busy":"2023-04-15T02:26:53.933060Z","iopub.status.idle":"2023-04-15T02:26:53.943073Z","shell.execute_reply":"2023-04-15T02:26:53.941514Z","shell.execute_reply.started":"2023-04-15T02:26:53.934056Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from tensorflow.keras import applications,layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n","from tensorflow.keras.layers.experimental.preprocessing import Resizing\n","from tensorflow.keras.layers.experimental.preprocessing import Normalization\n","import keras.backend as K"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:11:43.414982Z","iopub.status.busy":"2023-04-15T02:11:43.414528Z","iopub.status.idle":"2023-04-15T02:11:43.420102Z","shell.execute_reply":"2023-04-15T02:11:43.418934Z","shell.execute_reply.started":"2023-04-15T02:11:43.414944Z"},"trusted":true},"outputs":[],"source":["P = '../input/15-after-processed/15_after_processed/'"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:11:43.699367Z","iopub.status.busy":"2023-04-15T02:11:43.698782Z","iopub.status.idle":"2023-04-15T02:12:04.099856Z","shell.execute_reply":"2023-04-15T02:12:04.098606Z","shell.execute_reply.started":"2023-04-15T02:11:43.699309Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 21108 files belonging to 10019 classes.\n","Found 5414 files belonging to 4190 classes.\n"]}],"source":["train_image = tf.keras.utils.image_dataset_from_directory( P+'train_images/',\n","    labels=\"inferred\",\n","    label_mode=\"categorical\",\n","    class_names=None,\n","    color_mode=\"rgb\",\n","    batch_size=32,\n","    image_size=(224, 224),  \n",")\n","\n","val_image = tf.keras.utils.image_dataset_from_directory(  P+ 'val_images/',\n","    labels=\"inferred\",\n","    label_mode=\"categorical\",\n","    class_names=None, \n","    color_mode=\"rgb\",\n","    batch_size=32,\n","    image_size=(224, 224),\n",")\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:12:04.110666Z","iopub.status.busy":"2023-04-15T02:12:04.109768Z","iopub.status.idle":"2023-04-15T02:12:04.191374Z","shell.execute_reply":"2023-04-15T02:12:04.190244Z","shell.execute_reply.started":"2023-04-15T02:12:04.110626Z"},"trusted":true},"outputs":[],"source":["# Load the train and validation CSV files\n","train_csv = pd.read_csv(P+'train.csv')\n","val_csv = pd.read_csv(P+'val.csv')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:26:55.983390Z","iopub.status.busy":"2023-04-15T02:26:55.982664Z","iopub.status.idle":"2023-04-15T02:26:55.990462Z","shell.execute_reply":"2023-04-15T02:26:55.989261Z","shell.execute_reply.started":"2023-04-15T02:26:55.983342Z"},"trusted":true},"outputs":[],"source":["def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the number of classes in the training set\n","num_classes = len(train_image.class_names)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T02:26:58.014820Z","iopub.status.busy":"2023-04-15T02:26:58.014424Z","iopub.status.idle":"2023-04-15T02:45:54.569103Z","shell.execute_reply":"2023-04-15T02:45:54.567974Z","shell.execute_reply.started":"2023-04-15T02:26:58.014784Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","660/660 [==============================] - 93s 134ms/step - loss: 8.6555 - get_f1: 0.2854 - val_loss: 19.6565 - val_get_f1: 2.4510e-04\n","Epoch 2/10\n","660/660 [==============================] - 91s 137ms/step - loss: 0.4262 - get_f1: 0.9151 - val_loss: 21.7249 - val_get_f1: 2.4010e-04\n","Epoch 3/10\n","660/660 [==============================] - 90s 135ms/step - loss: 0.2447 - get_f1: 0.9614 - val_loss: 22.0594 - val_get_f1: 4.2481e-04\n","Epoch 4/10\n","660/660 [==============================] - 92s 139ms/step - loss: 0.1873 - get_f1: 0.9743 - val_loss: 22.2913 - val_get_f1: 2.2198e-04\n","Epoch 5/10\n","660/660 [==============================] - 90s 135ms/step - loss: 0.1588 - get_f1: 0.9791 - val_loss: 22.7313 - val_get_f1: 2.0640e-04\n","Epoch 6/10\n","660/660 [==============================] - 90s 135ms/step - loss: 0.1461 - get_f1: 0.9821 - val_loss: 22.9094 - val_get_f1: 2.3529e-04\n","Epoch 7/10\n","660/660 [==============================] - 92s 137ms/step - loss: 0.1497 - get_f1: 0.9801 - val_loss: 23.9067 - val_get_f1: 2.2624e-04\n","Epoch 8/10\n","660/660 [==============================] - 93s 139ms/step - loss: 0.2080 - get_f1: 0.9700 - val_loss: 25.4765 - val_get_f1: 2.0284e-04\n","Epoch 9/10\n","660/660 [==============================] - 90s 135ms/step - loss: 0.2242 - get_f1: 0.9690 - val_loss: 27.2120 - val_get_f1: 2.2198e-04\n","Epoch 10/10\n","660/660 [==============================] - 90s 135ms/step - loss: 0.2295 - get_f1: 0.9687 - val_loss: 28.0790 - val_get_f1: 3.9548e-04\n","170/170 [==============================] - 19s 107ms/step - loss: 28.0790 - get_f1: 4.0924e-04\n","Validation accuracy: 0.00040923754568211734\n"]}],"source":["# Load the ResNet50 model\n","resnet = tf.keras.applications.ResNet50(\n","    include_top=False,\n","    weights='imagenet',\n","    input_shape=(224, 224, 3),\n","    pooling='avg',\n",")\n","\n","# Freeze the layers of the ResNet50 model\n","for layer in resnet.layers:\n","    layer.trainable = False\n","\n","# Add a new output layer\n","output = tf.keras.layers.Dense(num_classes, activation='softmax')(resnet.output)\n","model = tf.keras.models.Model(inputs=resnet.input, outputs=output)\n","\n","# Compile the model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss='categorical_crossentropy',\n","    metrics=[get_f1],\n",")\n","\n","# Define a function to preprocess the labels in the validation set\n","def preprocess_labels(image, label):\n","    label = tf.argmax(label, axis=-1)\n","    label = tf.one_hot(label, num_classes)\n","    return image, label\n","\n","# Preprocess the labels in the validation set\n","val_image = val_image.map(preprocess_labels)\n","\n","# Train the model on the preprocessed training data\n","history = model.fit(\n","    train_image,\n","    validation_data=val_image,\n","    epochs=10,\n",")\n","\n","# Compute the accuracy of the predictions on the validation set\n","val_loss, val_acc = model.evaluate(val_image)\n","print('Validation accuracy:', val_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
