{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel, TFBertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tokenization \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import collections\n",
    "import langid\n",
    "import fasttext\n",
    "import string \n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_path = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'\n",
    "training_dataset =pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')\n",
    "testing_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# remove numbers\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def word_tokenize(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    return filtered_sentence\n",
    "\n",
    "# remove all preprocessing\n",
    "def remove_all_preprocessing(text):\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "# apply all preprocessing\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "testing_dataset['title'] = testing_dataset['title'].apply(lambda x: remove_all_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of \\\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.replace('\\\\', ''))\n",
    "# lower case\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = '../input/bert-base-uncased-220421/bert_base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_model(mname):\n",
    "    \n",
    "    idx = layers.Input((105), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = layers.Input((105), dtype=\"int32\", name=\"input_masks\")\n",
    "    \n",
    "    nlp = transformers.TFBertModel.from_pretrained(mname)\n",
    "    bert_out= nlp([idx, masks])[0]\n",
    "    \n",
    "    ## fine-tuning\n",
    "    x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "    x = layers.Dense(750, activation=\"swish\", name='text-embed')(x)\n",
    "    \n",
    "    model_title = Sequential([\n",
    "        Input(shape=(100,), name='title-input'),\n",
    "        Embedding(25000, 100, input_length=100, name='title-embed'),\n",
    "        Dropout(0.2),\n",
    "        Conv1D(300, 3, padding='valid', activation='relu', strides=1),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(720, name='title-dense1'), #650 -> 0.81\n",
    "        Activation('swish', name='title-act1'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(650, name='title-dense2'),\n",
    "        BatchNormalization(name='title-bn2'),\n",
    "        Activation('swish', name='title-act2'),\n",
    "    ], name='title-vec')\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "        n_classes = CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.7, \n",
    "        name='head/arc_margin', \n",
    "        dtype='float32'\n",
    "    )\n",
    "    \n",
    "    concatenate = Concatenate(name='concatenate')([x, model_title.output])\n",
    "    label = Input(shape=(), name='arc-input')\n",
    "    arc_face = margin([concatenate, label])\n",
    "    \n",
    "    output = Dense(CLASSES, activation='softmax', name='output')(arc_face)\n",
    "\n",
    "    # Compile model\n",
    "    model = tf.keras.Model(inputs=[idx, masks, model_title.input, label], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05237605  0.00266704 -0.0351623  ... -0.0220446   0.03120822\n",
      "   0.0482746 ]\n",
      " [ 0.03168393  0.0207246  -0.01772413 ... -0.02363313  0.00672482\n",
      "  -0.06600535]\n",
      " [-0.02553968  0.02782196  0.01990272 ... -0.0465625  -0.01983832\n",
      "   0.01210797]\n",
      " ...\n",
      " [-0.00043891  0.0663081  -0.00434749 ...  0.04577914  0.02965026\n",
      "  -0.03386611]\n",
      " [-0.02243138  0.02246639  0.05454137 ...  0.01867248  0.01682098\n",
      "   0.00408043]\n",
      " [-0.03517014  0.02272914  0.00684843 ... -0.00679976 -0.01856173\n",
      "  -0.00943737]]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(training_dataset['title'])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer,N_CLASSES,max_len=512):\n",
    "    tokens = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32)\n",
    "\n",
    "    y = tf.keras.layers.Input(shape=(),dtype=tf.int32)\n",
    "    bert = bert_layer.bert([tokens])\n",
    "    cls = bert.pooler_output \n",
    "    x = tf.keras.layers.BatchNormalization()(cls)\n",
    "    mar = margin([x,y])\n",
    "    output = tf.keras.layers.Softmax()(mar)\n",
    "    model = tf.keras.models.Model(inputs=[tokens,y],outputs=[output])\n",
    "    return model \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_classes = training_dataset[\"label_group\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m build_model(model,N_classes,max_len\u001b[39m=\u001b[39;49mmax_len)\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(\u001b[39m1e-5\u001b[39m),loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mcosine_similarity, metrics\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(bert_layer, N_CLASSES, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#masks = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#segments = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(),dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m----> 6\u001b[0m bert \u001b[39m=\u001b[39m bert_layer\u001b[39m.\u001b[39;49mbert([tokens])\n\u001b[1;32m      7\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39mpooler_output \n\u001b[1;32m      8\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mBatchNormalization()(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "model = build_model(model,N_classes,max_len=max_len)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),loss=tf.keras.losses.cosine_similarity, metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 610/610 [00:00<00:00, 550kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 539M/539M [00:08<00:00, 65.8MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 531/531 [00:00<00:00, 790kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 37.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 74.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 174kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "word_embedding_model = models.Transformer('sentence-transformers/distiluse-base-multilingual-cased-v2', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v2 were not used when initializing BertModel: ['transformer.layer.5.ffn.lin1.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at sentence-transformers/distiluse-base-multilingual-cased-v2 and are newly initialized: ['encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_version = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
    "model = BertModel.from_pretrained(bert_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>double tape m vhb mm x original double foam tape</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>maling tts canned pork luncheon meat gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>daster batik lengan pendek motif acak campur l...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>nescafe xcxclair latte ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>masker bahan kain spunbond non woven gsm ply l...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>mamypoko pants royal soft s popok celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>khanzaacc robot res mm subwoofer bass metal wi...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>kaldu non msg halal mama kamu ayam kampung sap...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>flex tape pelapis bocor isolasi ajaib anti bocor</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image   \n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  \\\n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title   \n",
       "0      94974f937d4c2433                          paper bag victoria secret  \\\n",
       "1      af3f9460c2838f0f   double tape m vhb mm x original double foam tape   \n",
       "2      b94cb00ed3e50f78            maling tts canned pork luncheon meat gr   \n",
       "3      8514fc58eafea283  daster batik lengan pendek motif acak campur l...   \n",
       "4      a6f319f924ad708c                          nescafe xcxclair latte ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  masker bahan kain spunbond non woven gsm ply l...   \n",
       "34246  be86851f72e2853c           mamypoko pants royal soft s popok celana   \n",
       "34247  ad27f0d08c0fcbf0  khanzaacc robot res mm subwoofer bass metal wi...   \n",
       "34248  e3b13bd1d896c05c  kaldu non msg halal mama kamu ayam kampung sap...   \n",
       "34249  af8bc4b2d2cf9083   flex tape pelapis bocor isolasi ajaib anti bocor   \n",
       "\n",
       "       label_group  \n",
       "0        249114794  \n",
       "1       2937985045  \n",
       "2       2395904891  \n",
       "3       4093212188  \n",
       "4       3648931069  \n",
       "...            ...  \n",
       "34245   3776555725  \n",
       "34246   2736479533  \n",
       "34247   4101248785  \n",
       "34248   1663538013  \n",
       "34249    459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 22:21:31.704802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 22:21:35.995097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='cahya/bert-base-indonesian-522M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 230k/230k [00:00<00:00, 59.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 156kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 62.0/62.0 [00:00<00:00, 78.2kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 468/468 [00:00<00:00, 616kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 545M/545M [00:15<00:00, 35.7MB/s] \n",
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_title_vectors = np.zeros((training_dataset.shape[0],768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3722it [14:58,  4.26it/s]"
     ]
    }
   ],
   "source": [
    "for idx,txt in tqdm(enumerate(training_dataset['title'])):\n",
    "  encoded_input = tokenizer(txt, return_tensors='tf')\n",
    "  output = model(encoded_input)\n",
    "  bert_title_vectors[idx]= output['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "encoded_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_titles = []\n",
    "for title in training_dataset['title']:\n",
    "    encoded_title = model.encode(title)\n",
    "    encoded_titles.append(torch.tensor(encoded_title))\n",
    "input_ids = pad_sequence(encoded_titles, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = torch.ones(input_ids.shape[0], input_ids.shape[1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "num_training_steps = (len(training_dataset) // batch_size) * num_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = torch.ones(input_ids.shape[0], input_ids.shape[1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, targets)\n\u001b[1;32m     21\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:62\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, features):\n\u001b[1;32m     61\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     trans_features \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m         trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "targets = torch.tensor(training_dataset['label_group'].values).to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs, masks = batch\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_training_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m progress_bar_train \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_training_steps))\n\u001b[1;32m      4\u001b[0m progress_bar_eval \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_epochs \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(eval_dataloader)))\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_training_steps' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(25 * len(eval_dataloader)))\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      progress_bar_train.update(1)\n",
    "\n",
    "  model.eval()\n",
    "  for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar_eval.update(1)\n",
    "    \n",
    "  print(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
